{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Code\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is used to train the model for the competition. The model is a simple CNN model with 3 convolutional layers and 2 fully connected layers. The model is trained on the training data and the trained model is saved to be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of contents**    \n",
    "- Model Training Code    \n",
    "  - Current image data    \n",
    "  - Image data    \n",
    "  - Final dataset    \n",
    "  - Steps for training the model    \n",
    "  - Data Cleaning & Preprocessing    \n",
    "    - Load the data    \n",
    "    - Preprocess the data    \n",
    "    - Data Augmentation    \n",
    "    - Data Normalization    \n",
    "    - Data Splitting    \n",
    "  - Model Building    \n",
    "    - Model Architecture    \n",
    "    - Model Compilation    \n",
    "  - Model Training    \n",
    "    - Model Training    \n",
    "  - Model Evaluation    \n",
    "    - Model Evaluation    \n",
    "    - Model Saving    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=false\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will train a model based on image data using tensorflow and yolo. The model will be trained on the dataset of images of the 4 different colors 2-d surfaces (Red, Green, Blue, Yellow) and will be used to predict the color of the surface in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current image data\n",
    "Currently, I have a dataset of 4 cubes, each broken down to categories:\n",
    "1. color\n",
    "2. drawing\n",
    "\n",
    "We are going to utilize only the color data for now. The color data is broken down into 4 categories:\n",
    "1. Red\n",
    "2. Green\n",
    "3. Blue\n",
    "4. Yellow\n",
    "\n",
    "The dataset is in the form of images of the 4 colors. The images are taken from different angles and lighting conditions to make the model robust to different conditions.\n",
    "\n",
    "After running this trial model, we will use the real dataset of cubes to train the model to detect real cubes.\n",
    "\n",
    ", we will use a pre-trained model and fine-tune it on the dataset of the 4 colors. We will use the YOLO model for this purpose.\n",
    "\n",
    "## Image data\n",
    "\n",
    "In addition to the base dataset, we will also use a dataset of real images of cubes. I've taken open source datasets of different cubes, such as rubik's cubes, and used them as the real dataset. We will use this dataset to train the model to detect real cubes.\n",
    "\n",
    "## Final dataset\n",
    "\n",
    "As we complete and test the model based off the\n",
    "base dataset and the real dataset, we will combine the two datasets to create the final dataset.\n",
    ", we will combine the base dataset and the real dataset (from accumulating pictures of the camera on the robotic arm) to create the final dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| \\## Images |\n",
    "|--|\n",
    "| !\\[Image 2\\](path/to/image2.jpg) |\n",
    "| !\\[Image 3\\](path/to/image3.jpg) |\n",
    "| !\\[Image 4\\](path/to/image4.jpg) |\n",
    "| !\\[Image 5\\](path/to/image5.jpg) |\n",
    "\n",
    "[https://github.com/Robjects-ML/SmartVision/blob/garzarobm/issue19/Setup/4_model_code/software-testing/jupyter-environments/model_training/data/training_first_iteration/raw_data%20(for_model_use)/resized-small/blue_color_icon_0322%20Small.png](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BOX_0017 Small.jpeg](<attachment:BOX_0017 Small.jpeg>) ![BOX_0018 Small.jpeg](<attachment:BOX_0018 Small.jpeg>) ![BOX_0020 Small.jpeg](<attachment:BOX_0020 Small.jpeg>) ![BOX_0021 Small.jpeg](<attachment:BOX_0021 Small.jpeg>) ![BOX_0304 Small.jpeg](<attachment:BOX_0304 Small.jpeg>) ![BOX_0310 Small.jpeg](<attachment:BOX_0310 Small.jpeg>) ![BOX_0311 Small.jpeg](<attachment:BOX_0311 Small.jpeg>) ![BOX_0312 Small.jpeg](<attachment:BOX_0312 Small.jpeg>) ![BOX_0314 Small.jpeg](<attachment:BOX_0314 Small.jpeg>) ![BOX_0315 Small.jpeg](<attachment:BOX_0315 Small.jpeg>) ![BOX_0316 Small.jpeg](<attachment:BOX_0316 Small.jpeg>) ![BOX_0317 Small.jpeg](<attachment:BOX_0317 Small.jpeg>) ![BOX_0318 Small.jpeg](<attachment:BOX_0318 Small.jpeg>) ![BOX_0319 Small.jpeg](<attachment:BOX_0319 Small.jpeg>) ![BOX_0320 Small.jpeg](<attachment:BOX_0320 Small.jpeg>) ![BOX_0321 Small.jpeg](<attachment:BOX_0321 Small.jpeg>) ![BOX_0322 Small.jpeg](<attachment:BOX_0322 Small.jpeg>) ![BOX_0323 Small.jpeg](<attachment:BOX_0323 Small.jpeg>) ![BOX_0324 Small.jpeg](<attachment:BOX_0324 Small.jpeg>) ![BOX_0325 Small.jpeg](<attachment:BOX_0325 Small.jpeg>) ![BOX_0326 Small.jpeg](<attachment:BOX_0326 Small.jpeg>) ![BOX_0327 Small.jpeg](<attachment:BOX_0327 Small.jpeg>) ![BOX_0328 Small.jpeg](<attachment:BOX_0328 Small.jpeg>) ![BOX_0329 Small.jpeg](<attachment:BOX_0329 Small.jpeg>) ![BOX_0330 Small.jpeg](<attachment:BOX_0330 Small.jpeg>) ![BOX_0331 Small.jpeg](<attachment:BOX_0331 Small.jpeg>) ![BOX_0332 Small.jpeg](<attachment:BOX_0332 Small.jpeg>) ![BOX_0333 Small.jpeg](<attachment:BOX_0333 Small.jpeg>) ![BOX_0334 Small.jpeg](<attachment:BOX_0334 Small.jpeg>) ![BOX_0335 Small.jpeg](<attachment:BOX_0335 Small.jpeg>) ![BOX_0336 Small.jpeg](<attachment:BOX_0336 Small.jpeg>) ![BOX_0337 Small.jpeg](<attachment:BOX_0337 Small.jpeg>) ![BOX_0338 Small.jpeg](<attachment:BOX_0338 Small.jpeg>) ![BOX_0339 Small.jpeg](<attachment:BOX_0339 Small.jpeg>) ![BOX_0340 Small.jpeg](<attachment:BOX_0340 Small.jpeg>) ![BOX_0341 Small.jpeg](<attachment:BOX_0341 Small.jpeg>) ![BOX_0342 Small.jpeg](<attachment:BOX_0342 Small.jpeg>) ![BOX_0343 Small.jpeg](<attachment:BOX_0343 Small.jpeg>) ![BOX_0344 Small.jpeg](<attachment:BOX_0344 Small.jpeg>) ![BOX_0345 Small.jpeg](<attachment:BOX_0345 Small.jpeg>) ![BOX_0346 Small.jpeg](<attachment:BOX_0346 Small.jpeg>) ![BOX_0347 Small.jpeg](<attachment:BOX_0347 Small.jpeg>) ![BOX_0348 Small.jpeg](<attachment:BOX_0348 Small.jpeg>) ![BOX_0349 Small.jpeg](<attachment:BOX_0349 Small.jpeg>) ![BOX_0350 Small.jpeg](<attachment:BOX_0350 Small.jpeg>) ![BOX_0351 Small.jpeg](<attachment:BOX_0351 Small.jpeg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for training the model\n",
    "1. Data Cleaning & Preprocessing\n",
    "    1. Load the data\n",
    "    2. Preprocess the data\n",
    "    3. Data Augmentation\n",
    "    4. Data Normalization\n",
    "    5. Data Splitting\n",
    "2. Model Building\n",
    "    1. Model Architecture\n",
    "    2. Model Compilation\n",
    "3. Model Training\n",
    "    1. Model Training\n",
    "4. Model Evaluation\n",
    "    1. Model Evaluation\n",
    "    2. Model Saving\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preprocessing\n",
    "The data is cleaned and preprocessed before training the model. The data is loaded, preprocessed, augmented, normalized, and split into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the data\n",
    "The data is loaded from the directory containing the images of the 4 colors. The images are loaded using the `ImageDataGenerator` class from the `tensorflow.keras.preprocessing.image` module. The images are loaded in batches of 32 and resized to 224x224 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the sample image\n",
    "image = cv2.imread('image.jpg')\n",
    "\n",
    "# Define new dimensions for resizing\n",
    "new_width, new_height = 224, 224  # Example dimensions\n",
    "\n",
    "# Resize the image\n",
    "resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "# Normalize pixel values\n",
    "normalized_image = image / 255.0\n",
    "\n",
    "# Rotate the image\n",
    "rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# Flip the image\n",
    "flipped_image = cv2.flip(image, 1)\n",
    "\n",
    "# Grayscale conversion\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur for noise reduction\n",
    "blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Threshold the image\n",
    "_, thresholded_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Edge detection using Canny\n",
    "edges = cv2.Canny(image, 100, 200)\n",
    "\n",
    "# Adjust brightness and contrast\n",
    "alpha, beta = 1.5, 10  # Contrast and brightness control\n",
    "adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "# Apply histogram equalization (improves contrast)\n",
    "gray_equalized = cv2.equalizeHist(gray_image)\n",
    "\n",
    "# Display the images using matplotlib\n",
    "images = {\n",
    "    'Original Image': cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "    'Resized Image': cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB),\n",
    "    'Normalized Image': normalized_image,\n",
    "    'Rotated Image': cv2.cvtColor(rotated_image, cv2.COLOR_BGR2RGB),\n",
    "    'Flipped Image': cv2.cvtColor(flipped_image, cv2.COLOR_BGR2RGB),\n",
    "    'Gray Image': gray_image,\n",
    "    'Blurred Image': cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB),\n",
    "    'Thresholded Image': thresholded_image,\n",
    "    'Edges': edges,\n",
    "    'Adjusted Image': cv2.cvtColor(adjusted_image, cv2.COLOR_BGR2RGB),\n",
    "    'Equalized Image': gray_equalized\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (title, img) in enumerate(images.items()):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    if len(img.shape) == 2:  # Grayscale image\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    else:  # Color image\n",
    "        plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Displaying Base Sample Images\n",
    "\n",
    "Below are the base sample images that will be used for training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Resize data for model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we can use the images in the model, we need to resize them to a fixed size. We will resize the images to 224x224 pixels, which is a common size used in many image classification models.\n",
    "\n",
    "\n",
    "The data is preprocessed by resizing the images to a fixed size and converting them to numpy arrays.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    # Resize the image\n",
    "    image = image.resize(target_size)\n",
    "    # Convert the image to a numpy array\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "# Preprocess the images\n",
    "base_sample_images_preprocessed = [preprocess_image(image_path) for image_path in base_sample_images]\n",
    "\n",
    "# Display the preprocessed images\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, image in enumerate(base_sample_images_preprocessed):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "## Load image labels from folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(filename.split('_')[0])\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder('images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot encoding\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical data into a format that can be provided to ML algorithms to do a better job in prediction. In one-hot encoding, each category is represented as a binary vector, where all elements are zero except for the element corresponding to the category, which is one.\n",
    "\n",
    "```python\n",
    "# Convert labels to one-hot encoding\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels_one_hot = label_binarizer.fit_transform(labels)\n",
    "```\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "## Data Augmentation - Expanding the Dataset with Existing Data\n",
    "\n",
    "\n",
    "\n",
    "In order to expand the dataset and make the model more robust, we will use data augmentation. Data augmentation is a technique used to artificially create new training data from the existing training data. This is done by applying random transformations to the existing training data, such as rotation, scaling, flipping, etc.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator object\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Augmentation - Expanding the Dataset manually with Simulated Data \n",
    "\n",
    "Ideally, we would like to have more data to train the model. This is especially important when the base dataset is small. If the camera on the robotic arm can take more pictures of the cubes, we can use these pictures to augment the base dataset. This will help in making the model more robust and generalizable.\n",
    "\n",
    "Using robotic arm to take more pictures of the cubes will will help the model more fine-tuned and likely improve the performance of the model against real data. Hopefully, this will help the model to generalize better to the real dataset. \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "For getting more data, we can use external data. We can use the data from the real dataset to augment the base dataset. This will help in making the model more robust and generalizable. This will prevent overfitting and improve the performance of the model against real data. Hopefully, this will help the model to generalize better to the real dataset.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation - Expanding the Dataset with External Data\n",
    "\n",
    "In addition to the base dataset, we can also use external data to augment the dataset. This can be done by using open-source datasets of cubes, and using them as the external data. This will help in making the model more robust and generalizable.\n",
    "\n",
    "For example, we can use the following open-source datasets of other cube-like objects with different colors, shapes, and sizes. I have choosen to compile open-source image datasets of different cubes, such as rubik's cubes, and use them as the external data. This will help in making the model more robust and generalizable. Finding other data that isn't so closely related to the base dataset will help the model to generalize better to the real dataset which could open up the possibility of predicting colors across similar object shapes and sizes.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def load_and_preprocess_images(image_paths, labels):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        images.append(image / 255.0)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Example usage\n",
    "image_paths = ['BOX_0017 Small.jpeg', 'BOX_0018 Small.jpeg', 'BOX_0020 Small.jpeg', 'BOX_0021 Small.jpeg']\n",
    "labels = ['Red', 'Green', 'Blue', 'Yellow']\n",
    "images, one_hot_labels = load_and_preprocess_images(image_paths, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the images\n",
    "base_sample_images = [\n",
    "    'BOX_0017 Small.jpeg',\n",
    "    'BOX_0018 Small.jpeg',\n",
    "    'BOX_0020 Small.jpeg',\n",
    "    'BOX_0021 Small.jpeg'\n",
    "]\n",
    "\n",
    "# Preprocess the images\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Preprocess the images\n",
    "base_sample_images = [preprocess_image(image_path) for image_path in base_sample_images]\n",
    "\n",
    "# Display the images\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, image in enumerate(base_sample_images):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use the YOLO model for training the model. The YOLO model is a state-of-the-art object detection model that is used for detecting objects in images. We will use a pre-trained YOLO model and fine-tune it on the dataset of the 4 colors. This en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Architecture\n",
    "\n",
    "```python\n",
    "# Load the pre-trained YOLO model\n",
    "yolo_model = tf.keras.applications.YOLOv3(weights='yolov3.h5')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Splitting \n",
    "\n",
    "```python\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Compilation\n",
    "\n",
    "```python\n",
    "# Compile the model\n",
    "yolo_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model is trained on the training data. The model is trained using the fit method of the model object.\n",
    "\n",
    "```python\n",
    "# Train the model\n",
    "yolo_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model is evaluated on the validation data. The model is evaluated using the evaluate method of the model object.\n",
    "\n",
    "```python\n",
    "# Evaluate the model\n",
    "yolo_model.evaluate(X_val, y_val)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Metrics Utilized & Rationale\n",
    "\n",
    "The model is evaluated on the validation data. The model is evaluated using the evaluate method of the model object.\n",
    "\n",
    "```python\n",
    "The data is loaded from the dataset of images of the classes of fruits. The data is loaded using the `load_data` function from the `data_loader.py` file. The data is loaded in the form of a dictionary with the keys as the class names and the values as the images of the respective classes.\n",
    "\n",
    "```python\n",
    "import data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Saving\n",
    "\n",
    "```python\n",
    "# Save the model\n",
    "yolo_model.save('yolo_model.h5')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
